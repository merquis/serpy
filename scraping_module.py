# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SERPY â€“ Scraper Google con Streamlit + Proxy (BrightData)
# VersiÃ³n 1.0.0 â€“ Optimizada, rÃ¡pida, sin duplicados ni JSON
# Autor: Merquis â€“ Abril 2025
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import streamlit as st                     # Para crear interfaces web interactivas
import urllib.request                      # Para realizar solicitudes HTTP con soporte para proxy
import urllib.parse                        # Para codificar parÃ¡metros de bÃºsqueda en URLs
from bs4 import BeautifulSoup              # Para analizar y extraer informaciÃ³n del HTML de Google

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ FUNCIONALIDAD: Scraping con mÃºltiples pÃ¡ginas
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def testear_proxy_google(query, num_results):
    """
    Realiza scraping de resultados de Google utilizando BrightData como proxy.
    Devuelve una lista de URLs de los primeros `num_results` resultados.
    """

    # Proxy configurado con credenciales de BrightData (zona serppy)
    proxy_url = 'http://brd-customer-hl_bdec3e3e-zone-serppy:o20gy6i0jgn4@brd.superproxy.io:33335'
    
    # Google muestra 10 resultados por pÃ¡gina
    step = 10

    # Lista donde se almacenarÃ¡n las URLs extraÃ­das
    urls = []

    # Recorremos las pÃ¡ginas necesarias hasta alcanzar `num_results`
    for start in range(0, num_results + step, step):
        if len(urls) >= num_results:
            break  # Si ya tenemos suficientes resultados, salimos

        # Codificamos la consulta para que sea compatible en la URL
        encoded_query = urllib.parse.quote(query)
        search_url = f'https://www.google.com/search?q={encoded_query}&start={start}'

        try:
            # Creamos un "opener" con proxy configurado
            opener = urllib.request.build_opener(
                urllib.request.ProxyHandler({
                    'http': proxy_url,
                    'https': proxy_url
                })
            )

            # Realizamos la solicitud a Google con un timeout amplio (90 segundos)
            response = opener.open(search_url, timeout=90)

            # Leemos y decodificamos el contenido HTML
            html = response.read().decode('utf-8', errors='ignore')

            # Analizamos el HTML con BeautifulSoup
            soup = BeautifulSoup(html, "html.parser")

            # Seleccionamos todos los enlaces que contienen un <h3> (resultados orgÃ¡nicos)
            enlaces_con_titulo = soup.select("a:has(h3)")

            # Recorremos los enlaces vÃ¡lidos y los agregamos a la lista
            for a in enlaces_con_titulo:
                if len(urls) >= num_results:
                    break  # Cortamos si alcanzamos el lÃ­mite deseado
                href = a.get('href')
                if href and href.startswith("http"):
                    urls.append(href)

        except Exception as e:
            # Si hay error en una pÃ¡gina (timeout, proxy, etc.), lo mostramos
            st.error(f"âŒ Error al conectar con start={start}: {str(e)}")
            continue

    # â–‘â–‘â–‘ Mostrar solo URLs en texto plano
    if urls:
        st.subheader("ğŸ”— Enlaces en texto plano")
        st.text("\n".join(urls))  # Mostramos las URLs separadas por saltos de lÃ­nea
    else:
        st.warning("âš ï¸ No se encontraron enlaces estructurados.")
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ–¥ï¸ INTERFAZ: GUI Streamlit con selector de resultados
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def render_scraping():
    """
    Interfaz de usuario para ejecutar el scraping.
    Permite al usuario escribir una bÃºsqueda y seleccionar el nÃºmero de resultados.
    """

    # TÃ­tulo principal
    st.title("ğŸ” Scraping de Google (multiresultado con start)")

    # Dos columnas: una para la bÃºsqueda, otra para el nÃºmero de resultados
    col1, col2 = st.columns([3, 1])

    with col1:
        # Campo para escribir la bÃºsqueda
        query = st.text_input("ğŸ” Escribe tu bÃºsqueda en Google")

    with col2:
        # Selector desplegable para elegir la cantidad de resultados
        num_results = st.selectbox("ğŸ“„ NÂº resultados", options=list(range(10, 101, 10)), index=0)

    # BotÃ³n que lanza la bÃºsqueda
    if st.button("Buscar") and query:
        with st.spinner("Consultando Google..."):
            testear_proxy_google(query, int(num_results))

