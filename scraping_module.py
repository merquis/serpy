# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SERPY â€“ Scraper Google con Streamlit + Proxy (BrightData)
# VersiÃ³n 1.0.0 â€“ Optimizada, rÃ¡pida, sin duplicados ni JSON
# Autor: Merquis â€“ Abril 2025
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import streamlit as st                     # Para crear interfaces interactivas en la web
import urllib.request                      # Para hacer solicitudes HTTP (con proxy configurado)
import urllib.parse                        # Para codificar parÃ¡metros en URLs
from bs4 import BeautifulSoup              # Para analizar y extraer contenido del HTML de Google

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ FUNCIONALIDAD: Scraping con mÃºltiples pÃ¡ginas
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def testear_proxy_google(query, num_results):
    """
    Realiza scraping de resultados de Google utilizando BrightData como proxy.
    Devuelve una lista de URLs de los primeros `num_results` resultados.
    """

    # ConfiguraciÃ³n del proxy BrightData (zona: serppy)
    proxy_url = 'http://brd-customer-hl_bdec3e3e-zone-serppy:o20gy6i0jgn4@brd.superproxy.io:33335'
    step = 10            # Google muestra 10 resultados por pÃ¡gina
    urls = []            # Lista para almacenar las URLs encontradas

    # Iteramos por las pÃ¡ginas de resultados necesarias para alcanzar `num_results`
    for start in range(0, num_results + step, step):
        if len(urls) >= num_results:
            break  # Si ya tenemos suficientes resultados, salimos del bucle

        # Codificamos la consulta para construir una URL segura
        encoded_query = urllib.parse.quote(query)
        search_url = f'https://www.google.com/search?q={encoded_query}&start={start}'

        try:
            # ConfiguraciÃ³n del "opener" para usar el proxy BrightData
            opener = urllib.request.build_opener(
                urllib.request.ProxyHandler({
                    'http': proxy_url,
                    'https': proxy_url
                })
            )

            # Realizamos la solicitud a Google con un timeout amplio (90 segundos)
            response = opener.open(search_url, timeout=90)

            # Leemos el HTML de la respuesta y lo convertimos en texto
            html = response.read().decode('utf-8', errors='ignore')

            # Usamos BeautifulSoup para analizar el HTML
            soup = BeautifulSoup(html, "html.parser")

            # Seleccionamos solo los enlaces que contienen un <h3> (resultados orgÃ¡nicos)
            enlaces_con_titulo = soup.select("a:has(h3)")

            # Recorremos los enlaces encontrados
            for a in enlaces_con_titulo:
                if len(urls) >= num_results:
                    break  # Cortamos si ya alcanzamos el nÃºmero solicitado

                href = a.get('href')
                if href and href.startswith("http"):
                    urls.append(href)  # Guardamos la URL si es vÃ¡lida

        except Exception as e:
            # En caso de error (por ejemplo, timeout), mostramos el mensaje en la interfaz
            st.error(f"âŒ Error al conectar con start={start}: {str(e)}")
            continue

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ”— VISUALIZACIÃ“N: Mostrar solo URLs en texto plano
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    if urls:
        st.subheader("ğŸ”— Enlaces en texto plano")        # TÃ­tulo de secciÃ³n
        st.text("\n".join(urls))                         # Mostramos todas las URLs en bloque
    else:
        st.warning("âš ï¸ No se encontraron enlaces estructurados.")  # En caso de no obtener nada
