"""scraping_module.py ‚Äì Top-10 URLs org√°nicas de Google ES v√≠a ScrapingAnt."""
from __future__ import annotations
import re, subprocess, sys
from urllib.parse import quote_plus
from typing import List, Tuple
import requests, streamlit as st

# ‚îÄ‚îÄ BeautifulSoup con instalaci√≥n din√°mica (solo la 1.¬™ vez) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
try:
    from bs4 import BeautifulSoup
except ModuleNotFoundError:
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "beautifulsoup4"])
        from bs4 import BeautifulSoup
    except Exception:
        BeautifulSoup = None   # fallback a regex

# ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOKEN        = st.secrets.get("scrapingant", {}).get("token", "")
GOOGLE       = "https://www.google.es/search?q="
MAX_RESULTS  = 10
TIMEOUT      = 20

# ‚îÄ‚îÄ Networking ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def fetch_html(query: str) -> Tuple[str | None, str | None]:
    """Devuelve (html, error). Si error != None, html ser√° None."""
    if not TOKEN:
        return None, "Falta el token de ScrapingAnt en secrets."

    search_url = GOOGLE + quote_plus(query)
    api_url    = "https://api.scrapingant.com/v2/general"

    try:
        r = requests.get(
            api_url,
            params={"url": search_url, "x-api-key": TOKEN},  # encoding correcto
            timeout=TIMEOUT,
        )
        r.raise_for_status()
        return r.text, None
    except requests.RequestException as e:
        return None, f"Error ScrapingAnt: {e}"

# ‚îÄ‚îÄ Parsing ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def parse_urls(html: str) -> List[str]:
    pattern = re.compile(r"/url\?q=(https?://[^&]+)&")
    urls: List[str] = []

    if BeautifulSoup:                                      # modo normal
        soup = BeautifulSoup(html, "html.parser")
        hrefs = (a.get("href", "") for a in soup.select("a"))
    else:                                                 # fallback regex
        hrefs = pattern.findall(html)

    for href in hrefs:
        match = pattern.match(href) if BeautifulSoup else (href,)
        if match:
            url = match[1] if BeautifulSoup else match[0]
            if "google." not in url and url not in urls:
                urls.append(url)
        if len(urls) >= MAX_RESULTS:
            break
    return urls

# ‚îÄ‚îÄ Streamlit UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def render() -> None:
    st.title("üîé Scraping Google (ScrapingAnt)")

    query = st.text_input("Frase de b√∫squeda")
    if st.button("Buscar") and query.strip():
        with st.spinner("Consultando Google‚Ä¶"):
            html, err = fetch_html(query.strip())

        if err:
            st.error(err)
            return

        urls = parse_urls(html)
        if not urls:
            st.warning("No se extrajeron URLs org√°nicas.")
            return

        st.success(f"Top {len(urls)} resultados")
        for i, u in enumerate(urls, 1):
            st.markdown(f"{i}. [{u}]({u})")

        st.download_button(
            "‚¨áÔ∏è Descargar CSV",
            data="\n".join(urls).encode(),
            file_name=f"google_{quote_plus(query)[:30]}.csv",
            mime="text/csv",
        )
