import streamlit as st
import urllib.request
import urllib.parse
import ssl
from bs4 import BeautifulSoup

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ FUNCIONALIDAD: Scraping estructural con tÃ­tulos y texto plano
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def testear_proxy_google(query):
    ssl._create_default_https_context = ssl._create_unverified_context

    proxy_url = 'http://brd-customer-hl_bdec3e3e-zone-serppy:o20gy6i0jgn4@brd.superproxy.io:33335'
    encoded_query = urllib.parse.quote(query)
    search_url = f'https://www.google.com/search?q={encoded_query}'

    st.write(f"ğŸ”— URL que se va a consultar: [{search_url}]({search_url})")

    try:
        opener = urllib.request.build_opener(
            urllib.request.ProxyHandler({
                'http': proxy_url,
                'https': proxy_url
            })
        )
        response = opener.open(search_url, timeout=30)
        html = response.read().decode('utf-8', errors='ignore')
        soup = BeautifulSoup(html, "html.parser")

        # â–‘â–‘â–‘ Extraer enlaces <a> que contienen <h3> (estructura de resultados)
        enlaces_con_titulo = soup.select("a:has(h3)")
        resultados = []
        raw_urls = []

        for a in enlaces_con_titulo:
            href = a.get("href")
            titulo = a.h3.get_text(strip=True) if a.h3 else ""
            if href and href.startswith("http"):
                resultados.append((titulo, href))
                raw_urls.append(href)

        raw_urls_unique = sorted(set(raw_urls))

        # â–‘â–‘â–‘ Mostrar resultados estructurados con tÃ­tulo + link
        if resultados:
            st.subheader("ğŸŒ Enlaces estructurados encontrados")
            for titulo, url in resultados:
                st.markdown(f"[{titulo}]({url})")
        else:
            st.warning("âš ï¸ No se encontraron enlaces estructurados en esta bÃºsqueda.")

        # â–‘â–‘â–‘ Mostrar solo las URLs en texto plano
        if raw_urls_unique:
            st.subheader("ğŸ”— Enlaces en texto plano (uno por lÃ­nea)")
            st.text("\n".join(raw_urls_unique))

        # â–‘â–‘â–‘ HTML completo para depuraciÃ³n
        with st.expander("ğŸ“„ Ver HTML completo de Google"):
            st.code(html, language='html')

    except Exception as e:
        st.error(f"âŒ Error al conectar vÃ­a proxy BrightData: {str(e)}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ–¥ï¸ INTERFAZ: GUI con Streamlit
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def render_scraping():
    st.title("ğŸ” Scraping de Google (estructural + texto plano)")

    col1, col2 = st.columns([3, 1])
    with col1:
        query = st.text_input("ğŸ” Escribe tu bÃºsqueda en Google")
    with col2:
        st.markdown("&nbsp;")  # Espaciado visual

    if st.button("Buscar") and query:
        with st.spinner("Consultando Google a travÃ©s del proxy..."):
            testear_proxy_google(query)
