# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SERPY â€“ Scraper Google con Streamlit + Proxy (BrightData)
# VersiÃ³n 1.0.0 â€“ RÃ¡pida, optimizada, sin JSON, sin duplicados
# Ãšltima modificaciÃ³n por Merquis â€“ Abril 2025
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import streamlit as st                     # Framework para construir interfaces web interactivas
import urllib.request                      # MÃ³dulo para realizar solicitudes HTTP
import urllib.parse                        # Para codificar correctamente los tÃ©rminos de bÃºsqueda
from bs4 import BeautifulSoup              # LibrerÃ­a para parsear y navegar HTML (scraping)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ” FUNCIONALIDAD PRINCIPAL: Scraping de Google paginado
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def testear_proxy_google(query, num_results):
    """
    Realiza scraping de Google utilizando proxy de BrightData.
    Obtiene los primeros `num_results` enlaces de resultados orgÃ¡nicos (con tÃ­tulo h3).
    """

    # URL del proxy BrightData (Zona: serppy)
    proxy_url = 'http://brd-customer-hl_bdec3e3e-zone-serppy:o20gy6i0jgn4@brd.superproxy.io:33335'

    step = 10              # NÃºmero de resultados que Google muestra por pÃ¡gina
    urls = []              # Lista final donde se almacenarÃ¡n las URLs extraÃ­das

    # PaginaciÃ³n para recorrer resultados (Google usa el parÃ¡metro start=)
    for start in range(0, num_results + step, step):
        if len(urls) >= num_results:
            break  # Si ya tenemos suficientes resultados, salimos del bucle

        # Codificar la bÃºsqueda para usarla en una URL
        encoded_query = urllib.parse.quote(query)
        search_url = f'https://www.google.com/search?q={encoded_query}&start={start}'

        try:
            # Configurar la conexiÃ³n con el proxy
            opener = urllib.request.build_opener(
                urllib.request.ProxyHandler({
                    'http': proxy_url,
                    'https': proxy_url
                })
            )

            # Hacer la solicitud con timeout amplio (90 segundos)
            response = opener.open(search_url, timeout=90)

            # Leer y parsear el HTML devuelto
            html = response.read().decode('utf-8', errors='ignore')
            soup = BeautifulSoup(html, "html.parser")

            # Buscar enlaces que contienen un <h3> (caracterÃ­stico de resultados orgÃ¡nicos)
            enlaces_con_titulo = soup.select("a:has(h3)")

            # Extraer los href de cada enlace
            for a in enlaces_con_titulo:
                if len(urls) >= num_results:
                    break  # Cortamos si alcanzamos el nÃºmero deseado
                href = a.get('href')
                if href and href.startswith("http"):
                    urls.append(href)

        except Exception as e:
            st.error(f"âŒ Error al conectar con start={start}: {str(e)}")
            continue  # Si hay error en una pÃ¡gina, pasamos a la siguiente

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ”— MOSTRAR RESULTADOS EN PANTALLA (sÃ³lo texto plano)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    if urls:
        st.subheader("ğŸ”— Enlaces en texto plano")
        st.text("\n".join(urls))
    else:
        st.warning("âš ï¸ No se encontraron enlaces estructurados.")
