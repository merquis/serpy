"""
Ejemplo de scraper genÃ©rico usando el mÃ³dulo reutilizable de Playwright
Este archivo demuestra cÃ³mo usar playwright_utils.py para crear nuevos scrapers
"""

import streamlit as st
import asyncio
from bs4 import BeautifulSoup
from modules.utils.playwright_utils import (
    PlaywrightConfig,
    obtener_html_simple,
    procesar_urls_en_lote,
    crear_config_generica
)


async def obtener_datos_pagina_generica(url: str) -> tuple:
    """
    Ejemplo de funciÃ³n para obtener datos de cualquier pÃ¡gina web.
    
    Args:
        url: URL de la pÃ¡gina a scrapear
        
    Returns:
        Tupla con (datos_extraidos, html_content)
    """
    # Crear configuraciÃ³n personalizada
    # Puedes ajustar estos parÃ¡metros segÃºn la pÃ¡gina que quieras scrapear
    config = PlaywrightConfig(
        wait_for_selector="body",  # Esperar a que el body estÃ© cargado
        timeout=30000,  # 30 segundos de timeout
        wait_until="domcontentloaded"  # Esperar solo a que el DOM estÃ© cargado
    )
    
    # TambiÃ©n puedes usar la funciÃ³n helper para crear una config
    # config = crear_config_generica(wait_for_selector="h1", timeout=45000)
    
    # Obtener HTML usando el mÃ³dulo reutilizable
    resultado, html = await obtener_html_simple(url, config)
    
    # Si hay error, retornar el error
    if resultado.get("error"):
        return resultado, ""
    
    # Parsear el HTML con BeautifulSoup
    soup = BeautifulSoup(html, "html.parser")
    
    # Extraer datos bÃ¡sicos como ejemplo
    datos_extraidos = {
        "url_original": url,
        "titulo": soup.find("title").text if soup.find("title") else "Sin tÃ­tulo",
        "h1": soup.find("h1").text if soup.find("h1") else "Sin H1",
        "meta_description": "",
        "num_imagenes": len(soup.find_all("img")),
        "num_enlaces": len(soup.find_all("a")),
        "parrafos": [p.text.strip() for p in soup.find_all("p")[:5]]  # Primeros 5 pÃ¡rrafos
    }
    
    # Extraer meta description si existe
    meta_desc = soup.find("meta", {"name": "description"})
    if meta_desc:
        datos_extraidos["meta_description"] = meta_desc.get("content", "")
    
    return datos_extraidos, html


async def procesar_multiples_paginas(urls: list) -> list:
    """
    Procesa mÃºltiples pÃ¡ginas web en lote.
    
    Args:
        urls: Lista de URLs para procesar
        
    Returns:
        Lista de resultados
    """
    # ConfiguraciÃ³n para procesamiento en lote
    config = PlaywrightConfig(
        wait_for_selector="body",
        timeout=30000,
        headless=True  # Ejecutar en modo headless para mejor rendimiento
    )
    
    # Procesar todas las URLs (mÃ¡ximo 3 concurrentes para no sobrecargar)
    resultados_html = await procesar_urls_en_lote(urls, config, max_concurrent=3)
    
    final_results = []
    
    for resultado, html in resultados_html:
        if resultado.get("error"):
            final_results.append(resultado)
        elif html:
            soup = BeautifulSoup(html, "html.parser")
            
            # Extraer datos bÃ¡sicos
            datos = {
                "url_original": resultado["url_original"],
                "titulo": soup.find("title").text if soup.find("title") else "Sin tÃ­tulo",
                "h1": soup.find("h1").text if soup.find("h1") else "Sin H1",
                "longitud_html": len(html),
                "exito": True
            }
            final_results.append(datos)
    
    return final_results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ Interfaz de Streamlit para demostraciÃ³n
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def render_scraping_generico():
    st.title("ğŸŒ Scraper GenÃ©rico con Playwright Reutilizable")
    
    st.markdown("""
    ### Ejemplo de uso del mÃ³dulo `playwright_utils.py`
    
    Este scraper demuestra cÃ³mo reutilizar el cÃ³digo de Playwright para crear nuevos scrapers.
    Puedes usar este ejemplo como base para crear scrapers para otras pÃ¡ginas web.
    """)
    
    # OpciÃ³n 1: Scraping de una sola URL
    st.subheader("ğŸ“„ Scraping de URL Individual")
    
    url_individual = st.text_input(
        "Ingresa una URL para scrapear:",
        value="https://example.com"
    )
    
    if st.button("ğŸ” Scrapear URL Individual"):
        with st.spinner("Obteniendo datos..."):
            datos, html = asyncio.run(obtener_datos_pagina_generica(url_individual))
        
        if datos.get("error"):
            st.error(f"Error: {datos['error']} - {datos.get('details', '')}")
        else:
            st.success("âœ… Datos obtenidos exitosamente")
            st.json(datos)
            
            if html:
                st.download_button(
                    "â¬‡ï¸ Descargar HTML",
                    data=html.encode("utf-8"),
                    file_name="pagina_scrapeada.html",
                    mime="text/html"
                )
    
    # OpciÃ³n 2: Scraping de mÃºltiples URLs
    st.subheader("ğŸ“š Scraping de MÃºltiples URLs")
    
    urls_multiples = st.text_area(
        "Ingresa varias URLs (una por lÃ­nea):",
        value="https://example.com\nhttps://example.org",
        height=100
    )
    
    if st.button("ğŸ” Scrapear MÃºltiples URLs"):
        urls = [url.strip() for url in urls_multiples.split("\n") if url.strip()]
        
        if urls:
            with st.spinner(f"Procesando {len(urls)} URLs..."):
                resultados = asyncio.run(procesar_multiples_paginas(urls))
            
            st.success(f"âœ… Procesadas {len(resultados)} URLs")
            
            # Mostrar resultados en tabla
            for i, resultado in enumerate(resultados):
                with st.expander(f"Resultado {i+1}: {resultado.get('url_original', 'URL')}"):
                    st.json(resultado)
    
    # SecciÃ³n de ayuda
    with st.expander("ğŸ’¡ CÃ³mo adaptar este cÃ³digo para tu scraper"):
        st.markdown("""
        ### Pasos para crear tu propio scraper:
        
        1. **Importa las utilidades de Playwright:**
        ```python
        from modules.utils.playwright_utils import (
            PlaywrightConfig,
            obtener_html_simple,
            procesar_urls_en_lote
        )
        ```
        
        2. **Configura segÃºn tus necesidades:**
        ```python
        config = PlaywrightConfig(
            wait_for_selector="#mi-selector",  # Selector especÃ­fico de tu pÃ¡gina
            timeout=45000,  # Ajusta el timeout segÃºn necesites
            wait_until="networkidle"  # O "domcontentloaded" para pÃ¡ginas mÃ¡s rÃ¡pidas
        )
        ```
        
        3. **ObtÃ©n el HTML:**
        ```python
        resultado, html = await obtener_html_simple(url, config)
        ```
        
        4. **Parsea con BeautifulSoup:**
        ```python
        soup = BeautifulSoup(html, "html.parser")
        # Extrae los datos que necesites
        ```
        
        ### Configuraciones disponibles:
        - `headless`: True/False (modo sin ventana)
        - `timeout`: Tiempo mÃ¡ximo de espera en ms
        - `wait_until`: "load", "domcontentloaded", "networkidle"
        - `user_agent`: Personaliza el User-Agent
        - `wait_for_selector`: Espera a que aparezca un elemento especÃ­fico
        - `extra_headers`: Headers HTTP adicionales
        """)


if __name__ == "__main__":
    render_scraping_generico()
