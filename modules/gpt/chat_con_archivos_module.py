# modules/gpt/chat_con_archivos_module.py

import streamlit as st
from openai import OpenAI
import io # Para manejar bytes de archivos en memoria

# Importa las librer√≠as necesarias para procesar archivos
try:
    import PyPDF2 # pip install PyPDF2
except ImportError:
    st.error("Librer√≠a PyPDF2 no encontrada. Por favor, inst√°lala: pip install PyPDF2")
    PyPDF2 = None # Para evitar m√°s errores si no est√°

try:
    from docx import Document # pip install python-docx
except ImportError:
    st.error("Librer√≠a python-docx no encontrada. Por favor, inst√°lala: pip install python-docx")
    Document = None # Para evitar m√°s errores


def procesar_archivo_subido(uploaded_file):
    contenido = ""
    if uploaded_file is not None:
        bytes_data = uploaded_file.getvalue()
        
        if uploaded_file.type == "text/plain":
            try:
                contenido = bytes_data.decode("utf-8")
            except UnicodeDecodeError:
                try:
                    contenido = bytes_data.decode("latin-1") # Intenta con otra codificaci√≥n com√∫n
                except Exception as e:
                    st.error(f"Error al decodificar archivo de texto: {e}")
                    return None
        elif uploaded_file.type == "application/pdf":
            if PyPDF2 is None: return None # Si la librer√≠a no se import√≥
            try:
                pdf_reader = PyPDF2.PdfReader(io.BytesIO(bytes_data))
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    contenido += page.extract_text() or "" # A√±adir 'or ""' por si extract_text devuelve None
            except Exception as e:
                st.error(f"Error al procesar PDF: {e}")
                return None
        elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            if Document is None: return None # Si la librer√≠a no se import√≥
            try:
                doc = Document(io.BytesIO(bytes_data))
                for para in doc.paragraphs:
                    contenido += para.text + "\n"
            except Exception as e:
                st.error(f"Error al procesar DOCX: {e}")
                return None
        else:
            st.warning(f"Tipo de archivo '{uploaded_file.type}' no soportado para extracci√≥n de texto directa.")
            return None
        
        if not contenido.strip():
            st.warning("No se pudo extraer texto del archivo o el archivo est√° vac√≠o.")
            return None
            
        return contenido
    return None

def render_chat_con_archivos():
    st.header("üí¨ Chat con tus Archivos")
    st.markdown("Sube un archivo (TXT, PDF, DOCX) y haz preguntas sobre su contenido.")

    try:
        client = OpenAI(api_key=st.secrets["openai"]["api_key"])
    except Exception as e:
        st.error(f"Error al inicializar OpenAI: {e}. Configura 'openai.api_key'.")
        return

    if "chat_archivos_history" not in st.session_state:
        st.session_state.chat_archivos_history = []
    if "chat_archivos_contenido" not in st.session_state:
        st.session_state.chat_archivos_contenido = None
    if "chat_archivos_nombre" not in st.session_state:
        st.session_state.chat_archivos_nombre = None

    with st.sidebar:
        st.subheader("Archivo para Chat")
        uploaded_file = st.file_uploader(
            "Sube un archivo", 
            type=["txt", "pdf", "docx"], 
            key="uploader_chat_archivos"
        )

        if uploaded_file is not None:
            if st.session_state.chat_archivos_nombre != uploaded_file.name: # Nuevo archivo o cambio de archivo
                with st.spinner(f"Procesando '{uploaded_file.name}'..."):
                    contenido = procesar_archivo_subido(uploaded_file)
                    if contenido:
                        st.session_state.chat_archivos_contenido = contenido
                        st.session_state.chat_archivos_nombre = uploaded_file.name
                        st.session_state.chat_archivos_history = [] # Resetear historial para nuevo archivo
                        st.success(f"'{uploaded_file.name}' procesado. ¬°Listo para tus preguntas!")
                    else:
                        st.session_state.chat_archivos_contenido = None # Asegurar que est√© limpio si falla
                        st.session_state.chat_archivos_nombre = None
                        st.error(f"No se pudo extraer contenido de '{uploaded_file.name}'.")
        
        if st.session_state.chat_archivos_nombre:
            st.info(f"Archivo cargado: **{st.session_state.chat_archivos_nombre}**")
            if st.button("Quitar archivo cargado", key="sidebar_chat_archivos_quitar"):
                st.session_state.chat_archivos_contenido = None
                st.session_state.chat_archivos_nombre = None
                st.session_state.chat_archivos_history = []
                st.rerun()
        
        st.markdown("---")
        modelos_archivos = ["gpt-4o", "gpt-3.5-turbo-16k", "gpt-4-turbo"] # Modelos con contexto m√°s largo son mejores
        modelo_seleccionado_archivos = st.selectbox(
            "ü§ñ Elige el modelo (Chat con Archivos)", 
            modelos_archivos, 
            index=0, 
            key="sidebar_chat_archivos_model_select",
            help="Modelos con mayor ventana de contexto como gpt-4o o gpt-4-turbo son recomendados."
        )


    chat_container_archivos = st.container(height=450)
    with chat_container_archivos:
        for mensaje in st.session_state.chat_archivos_history:
            with st.chat_message(mensaje["role"]):
                st.markdown(mensaje['content'])

    if prompt := st.chat_input("Haz una pregunta sobre el archivo..."):
        if st.session_state.chat_archivos_contenido is None:
            st.warning("Por favor, sube y procesa un archivo primero.")
        else:
            st.session_state.chat_archivos_history.append({"role": "user", "content": prompt})
            with chat_container_archivos:
                with st.chat_message("user"):
                    st.markdown(prompt)

            with st.spinner("GPT (Chat con Archivos) est√° pensando..."):
                try:
                    # Preparamos el historial para la API, incluyendo el system prompt con el contexto del archivo
                    mensajes_para_api = [
                        {"role": "system", 
                         "content": f"Eres un asistente experto en responder preguntas bas√°ndote en el contenido de un documento proporcionado. El documento se llama '{st.session_state.chat_archivos_nombre}'. A continuaci√≥n, se presenta el contenido del documento:\n\n---INICIO DEL CONTENIDO DEL ARCHIVO---\n{st.session_state.chat_archivos_contenido}\n---FIN DEL CONTENIDO DEL ARCHIVO---\n\nResponde las preguntas del usuario bas√°ndote √öNICAMENTE en este contenido. Si la respuesta no se encuentra en el texto, ind√≠calo claramente."}
                    ]
                    # A√±adir historial de la conversaci√≥n actual sobre el archivo
                    for msg in st.session_state.chat_archivos_history:
                        if msg["role"] != "system": # Evitar duplicar system prompts si se guardaran
                             mensajes_para_api.append({"role": msg["role"], "content": msg["content"]})
                    
                    # Asegurarse de que el √∫ltimo mensaje sea el prompt del usuario si no se incluy√≥ arriba
                    if mensajes_para_api[-1]["role"] == "assistant":
                         mensajes_para_api.append({"role": "user", "content": prompt})


                    response = client.chat.completions.create(
                        model=modelo_seleccionado_archivos,
                        messages=mensajes_para_api,
                        stream=True
                    )
                    
                    with chat_container_archivos:
                        with st.chat_message("assistant"):
                            full_response_content = st.write_stream(response)
                    
                    st.session_state.chat_archivos_history.append({"role": "assistant", "content": full_response_content})

                except Exception as e:
                    st.error(f"‚ùå Error con OpenAI (Chat con Archivos): {e}")
                    error_msg = f"Error de API: {e}"
                    st.session_state.chat_archivos_history.append({"role": "assistant", "content": error_msg})
                    with chat_container_archivos:
                        with st.chat_message("assistant"):
                            st.error(error_msg)
