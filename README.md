# üìã GU√çA COMPLETA DE LA INTERFAZ SERPY

## üóÇÔ∏è ESTRUCTURA GENERAL

### Archivo Principal
- **Archivo:** `streamlit_app.py`
- **Clase principal:** `SerpyApp`
- **Funci√≥n de navegaci√≥n:** `render_navigation_menu()`

---

## üß≠ SECCI√ìN NAVEGACI√ìN

### üîç Scraping

#### 1. **URLs de Google** (Bot√≥n Rojo) - Sistema 4 en 1
- **Archivo UI:** `ui/pages/google_scraping.py`
- **Clase:** `GoogleScrapingPage`
- **Servicio:** `services/google_scraping_service.py`
- **Clase del servicio:** `GoogleScrapingService`
- **Funci√≥n principal:** `search_multiple_queries()`
- **Funci√≥n del bot√≥n:** Se ejecuta con `_perform_search()` que llama a `search_multiple_queries()`
- **Funcionalidades (checkboxes):** 
  1. **"üè∑Ô∏è Extraer etiquetas HTML autom√°ticamente"**: Extrae H1/H2/H3 de las URLs encontradas
  2. **"üìä Ejecutar an√°lisis sem√°ntico"** (requiere etiquetas): Agrupa y optimiza las etiquetas usando IA
  3. **"üìù Generar art√≠culo JSON"** (requiere etiquetas): Genera art√≠culos SEO completos
- **Flujos posibles:**
  - **Solo b√∫squeda**: busca URLs ‚Üí guarda en MongoDB colecci√≥n "URLs Google"
  - **Con etiquetas**: busca URLs ‚Üí extrae etiquetas ‚Üí guarda en MongoDB colecci√≥n "hoteles"
  - **Con an√°lisis sem√°ntico**: busca URLs ‚Üí extrae etiquetas ‚Üí an√°lisis sem√°ntico ‚Üí √°rbol SEO optimizado
  - **Completo (4 en 1)**: busca URLs ‚Üí extrae etiquetas ‚Üí an√°lisis sem√°ntico ‚Üí genera art√≠culos ‚Üí guarda en MongoDB colecci√≥n "posts"
- **Servicios reutilizados:**
  - `TagScrapingService` para extracci√≥n de etiquetas
  - `EmbeddingsService` para an√°lisis sem√°ntico
  - `ArticleGeneratorService` para generaci√≥n de art√≠culos
- **Para llamar desde otros m√≥dulos:**
  ```python
  from services.google_scraping_service import GoogleScrapingService
  service = GoogleScrapingService()
  results = service.search_multiple_queries(queries, num_results, language_code, region_code, google_domain)
  ```

#### 2. **Etiquetas HTML**
- **Archivo UI:** `ui/pages/tag_scraping.py`
- **Clase:** `TagScrapingPage`
- **Servicio:** `services/tag_scraping_service.py`
- **Clase del servicio:** `TagScrapingService`
- **Funci√≥n principal:** `scrape_tags_from_json()`
- **Funci√≥n del bot√≥n:** Se ejecuta con `_process_urls()` que llama a `scrape_tags_from_json()`
- **Fuentes de datos:**
  - Desde Drive (carpeta "scraping google")
  - Desde ordenador (archivo JSON)
  - **Desde MongoDB** (colecci√≥n "URLs Google") - muestra los √∫ltimos 50 documentos con ID completo
- **Para llamar desde otros m√≥dulos:**
  ```python
  from services.tag_scraping_service import TagScrapingService
  service = TagScrapingService()
  results = await service.scrape_tags_from_json(json_data, max_concurrent, progress_callback)
  ```

#### 3. **URLs manuales**
- **Archivo UI:** `ui/pages/manual_scraping.py`
- **Clase:** `ManualScrapingPage`
- **Servicio:** `services/manual_scraping_service.py`
- **Clase del servicio:** `ManualScrapingService`
- **Funci√≥n principal:** `scrape_urls()`
- **Funci√≥n del bot√≥n:** Se ejecuta con `_execute_scraping()` que llama a `scrape_urls()`
- **Para llamar desde otros m√≥dulos:**
  ```python
  from services.manual_scraping_service import ManualScrapingService
  service = ManualScrapingService()
  results = service.scrape_urls(urls, tags, max_workers, timeout)
  ```

#### 4. **Booking.com**
- **Archivo UI:** `ui/pages/booking_scraping.py`
- **Clase:** `BookingScrapingPage`
- **Servicio:** `services/booking_scraping_service.py`
- **Clase del servicio:** `BookingScrapingService`
- **Funci√≥n principal:** `scrape_hotels()`
- **Funci√≥n del bot√≥n:** Se ejecuta con `_perform_scraping()` que llama a `scrape_hotels()`
- **Para llamar desde otros m√≥dulos:**
  ```python
  from services.booking_scraping_service import BookingScrapingService
  service = BookingScrapingService()
  results = await service.scrape_hotels(booking_urls, progress_callback)
  ```

---

## üìù SECCI√ìN CONTENIDO

#### 5. **Generador de art√≠culos**
- **Archivo UI:** `ui/pages/article_generator.py`
- **Clase:** `ArticleGeneratorPage`
- **Servicio:** `services/article_generator_service.py`
- **Clase del servicio:** `ArticleGeneratorService`
- **Funci√≥n principal:** `generate_article_schema()`
- **Funci√≥n del bot√≥n:** Se ejecuta con `_execute_generation()` que llama a `generate_article_schema()`
- **Para llamar desde otros m√≥dulos:**
  ```python
  from services.article_generator_service import ArticleGeneratorService
  service = ArticleGeneratorService()
  result = service.generate_article_schema(keyword, language, content_type, model, generate_text, generate_slug, competition_data, temperature, top_p, frequency_penalty, presence_penalty)
  ```

#### 6. **Chat GPT**
- **Archivo UI:** `ui/pages/gpt_chat.py`
- **Clase:** `GPTChatPage`
- **Servicio:** `services/chat_service.py`
- **Clase del servicio:** `ChatService`
- **Funci√≥n principal:** `generate_response()`
- **Funci√≥n del bot√≥n:** Se ejecuta autom√°ticamente al escribir en el chat input que llama a `generate_response()`
- **Para llamar desde otros m√≥dulos:**
  ```python
  from services.chat_service import ChatService
  service = ChatService()
  response = service.generate_response(messages, model, temperature, max_tokens, stream)
  ```

---

## üìä SECCI√ìN AN√ÅLISIS

#### 7. **An√°lisis sem√°ntico**
- **Archivo UI:** `ui/pages/embeddings_analysis.py`
- **Clase:** `EmbeddingsAnalysisPage`
- **Servicio:** `services/embeddings_service.py`
- **Clase del servicio:** `EmbeddingsService`
- **Funci√≥n principal:** `analyze_and_group_titles()`
- **Funci√≥n del bot√≥n:** Se ejecuta con `_execute_analysis()` que llama a `analyze_and_group_titles()`
- **Para llamar desde otros m√≥dulos:**
  ```python
  from services.embeddings_service import EmbeddingsService
  service = EmbeddingsService()
  result = service.analyze_and_group_titles(data, max_titles_h2, max_titles_h3, n_clusters_h2, n_clusters_h3, model)
  ```

---

## üîß SERVICIOS AUXILIARES

### Servicios de utilidad:
- **Drive:** `services/drive_service.py` - `DriveService`
- **MongoDB:** `repositories/mongo_repository.py` - `MongoRepository`
- **HTTP:** `services/utils/httpx_service.py` - `HttpxService`
- **Playwright:** `services/utils/playwright_service.py` - `PlaywrightService`

### Componentes UI comunes:
- **Archivo:** `ui/components/common.py`
- **Componentes:** `Alert`, `Button`, `Card`, `LoadingSpinner`, `DataDisplay`, etc.

---

## üìã RESUMEN DE FUNCIONES PRINCIPALES

| Bot√≥n | Funci√≥n Principal | Servicio | M√©todo |
|-------|------------------|----------|---------|
| URLs de Google | `search_multiple_queries()` | `GoogleScrapingService` | Scraping con BrightData API |
| Etiquetas HTML | `scrape_tags_from_json()` | `TagScrapingService` | Extracci√≥n de H1/H2/H3 |
| URLs manuales | `scrape_urls()` | `ManualScrapingService` | Scraping de etiquetas SEO |
| Booking.com | `scrape_hotels()` | `BookingScrapingService` | Extracci√≥n de datos de hoteles |
| Generador de art√≠culos | `generate_article_schema()` | `ArticleGeneratorService` | Generaci√≥n con GPT |
| Chat GPT | `generate_response()` | `ChatService` | Chat libre con GPT |
| An√°lisis sem√°ntico | `analyze_and_group_titles()` | `EmbeddingsService` | Agrupaci√≥n con embeddings |

---

## üéØ EJEMPLO DE USO DESDE OTROS M√ìDULOS

```python
# Ejemplo para usar el scraping de Google desde otro m√≥dulo
from services.google_scraping_service import GoogleScrapingService

def mi_funcion_personalizada():
    service = GoogleScrapingService()
    
    # Configurar par√°metros
    queries = ["hoteles Madrid", "restaurantes Barcelona"]
    num_results = 20
    language_code = "es"
    region_code = "es"
    google_domain = "google.es"
    
    # Ejecutar scraping
    results = service.search_multiple_queries(
        queries=queries,
        num_results=num_results,
        language_code=language_code,
        region_code=region_code,
        google_domain=google_domain
    )
    
    return results
```

---

## üèóÔ∏è ARQUITECTURA Y REUTILIZACI√ìN DE C√ìDIGO

### Principios de dise√±o:
1. **DRY (Don't Repeat Yourself)**: No hay duplicaci√≥n de c√≥digo
2. **Separaci√≥n de responsabilidades**: L√≥gica de negocio (servicios) separada de la UI (p√°ginas)
3. **Reutilizaci√≥n de componentes**: Las p√°ginas pueden importar y usar m√©todos de otras p√°ginas

### Ejemplo de reutilizaci√≥n:
La p√°gina "URLs de Google" cuando tiene el checkbox marcado:
- **L√≥gica**: Usa `TagScrapingService.scrape_tags_from_json()` (mismo servicio que Etiquetas HTML)
- **Visualizaci√≥n**: Importa `TagScrapingPage` y usa su m√©todo `_display_url_result()`

```python
# En GoogleScrapingPage.__init__()
from ui.pages.tag_scraping import TagScrapingPage
self.tag_page = TagScrapingPage()

# En GoogleScrapingPage._display_url_result()
def _display_url_result(self, url_result: Dict[str, Any]):
    """Reutiliza el m√©todo de visualizaci√≥n de la p√°gina de etiquetas HTML"""
    self.tag_page._display_url_result(url_result)
```

### Flujos de datos entre m√≥dulos:
1. **URLs de Google ‚Üí MongoDB (colecci√≥n "URLs Google")**
2. **Etiquetas HTML ‚Üí puede cargar desde MongoDB (colecci√≥n "URLs Google")**
3. **URLs de Google con etiquetas ‚Üí MongoDB (colecci√≥n "hoteles")**
4. **URLs de Google completo (4 en 1) ‚Üí MongoDB (colecci√≥n "posts")**

### Sistema 4 en 1 de Google Scraping:
El m√≥dulo "URLs de Google" ahora es un sistema completo que puede ejecutar hasta 4 procesos en cadena:

1. **B√∫squeda en Google** (siempre se ejecuta)
   - Usa BrightData API para obtener URLs de los resultados
   - Soporta m√∫ltiples b√∫squedas separadas por comas

2. **Extracci√≥n de etiquetas HTML** (checkbox opcional)
   - Reutiliza `TagScrapingService`
   - Extrae H1/H2/H3 de todas las URLs encontradas
   - Usa HTTPX con fallback a Playwright para sitios con JavaScript

3. **An√°lisis sem√°ntico** (checkbox opcional, requiere etiquetas)
   - Reutiliza `EmbeddingsService`
   - Agrupa H2s y H3s similares usando embeddings de OpenAI
   - Genera un √°rbol SEO optimizado con H1 generado por IA
   - Asocia H3s a sus H2s m√°s relevantes sem√°nticamente

4. **Generaci√≥n de art√≠culos** (checkbox opcional, requiere etiquetas)
   - Reutiliza `ArticleGeneratorService`
   - Genera un art√≠culo por cada keyword de b√∫squeda
   - Si el an√°lisis sem√°ntico est√° activo, usa el √°rbol optimizado
   - Si no, usa las etiquetas en bruto como datos de competencia
   - Guarda autom√°ticamente en MongoDB (colecci√≥n "posts")

### Ejemplo de flujo completo:
```
B√∫squeda: "hoteles Madrid, restaurantes Barcelona"
‚Üì
1. Obtiene 30 URLs de Google para cada b√∫squeda
‚Üì
2. Extrae etiquetas HTML de las 60 URLs
‚Üì
3. An√°lisis sem√°ntico: agrupa y optimiza la estructura
‚Üì
4. Genera 2 art√≠culos SEO (uno por keyword) usando el √°rbol optimizado
‚Üì
Resultado: 2 art√≠culos guardados en MongoDB
```

---

## üìÅ ESTRUCTURA DEL PROYECTO

```
serpy/
‚îú‚îÄ‚îÄ config/              # Configuraci√≥n y settings
‚îú‚îÄ‚îÄ repositories/        # Capa de acceso a datos
‚îú‚îÄ‚îÄ services/           # L√≥gica de negocio
‚îÇ   ‚îú‚îÄ‚îÄ utils/         # Utilidades (httpx, playwright)
‚îÇ   ‚îî‚îÄ‚îÄ ...            # Servicios espec√≠ficos
‚îú‚îÄ‚îÄ ui/                # Interfaz de usuario
‚îÇ   ‚îú‚îÄ‚îÄ components/    # Componentes reutilizables
‚îÇ   ‚îî‚îÄ‚îÄ pages/         # P√°ginas de la aplicaci√≥n
‚îú‚îÄ‚îÄ streamlit_app.py   # Punto de entrada
‚îú‚îÄ‚îÄ requirements.txt   # Dependencias
‚îî‚îÄ‚îÄ Dockerfile        # Configuraci√≥n Docker
```

Esta gu√≠a te proporciona toda la informaci√≥n que necesitas para entender la estructura de tu aplicaci√≥n y c√≥mo llamar a las funciones desde otros m√≥dulos.
