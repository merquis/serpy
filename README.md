# üìã GU√çA COMPLETA DE LA INTERFAZ SERPY

## üóÇÔ∏è ESTRUCTURA GENERAL

### Archivo Principal
- **Archivo:** `streamlit_app.py`
- **Clase principal:** `SerpyApp`
- **Funci√≥n de navegaci√≥n:** `render_navigation_menu()`

---

## üß≠ SECCI√ìN NAVEGACI√ìN

### üîç Scraping

#### 1. **URLs de Google** (Bot√≥n Rojo)
- **Archivo UI:** `ui/pages/google_scraping.py`
- **Clase:** `GoogleScrapingPage`
- **Servicio:** `services/google_scraping_service.py`
- **Clase del servicio:** `GoogleScrapingService`
- **Funci√≥n principal:** `search_multiple_queries()`
- **Funci√≥n del bot√≥n:** Se ejecuta con `_perform_search()` que llama a `search_multiple_queries()`
- **Funcionalidad adicional:** 
  - **Checkbox "üè∑Ô∏è Extraer etiquetas HTML autom√°ticamente"**: Ejecuta un flujo 2 en 1
  - Cuando est√° marcado: busca URLs ‚Üí extrae etiquetas H1/H2/H3 ‚Üí guarda en MongoDB colecci√≥n "hoteles"
  - Cuando NO est√° marcado: busca URLs ‚Üí guarda en MongoDB colecci√≥n "URLs Google"
  - Reutiliza la l√≥gica y visualizaci√≥n de `TagScrapingPage`
- **Para llamar desde otros m√≥dulos:**
  ```python
  from services.google_scraping_service import GoogleScrapingService
  service = GoogleScrapingService()
  results = service.search_multiple_queries(queries, num_results, language_code, region_code, google_domain)
  ```

#### 2. **Etiquetas HTML**
- **Archivo UI:** `ui/pages/tag_scraping.py`
- **Clase:** `TagScrapingPage`
- **Servicio:** `services/tag_scraping_service.py`
- **Clase del servicio:** `TagScrapingService`
- **Funci√≥n principal:** `scrape_tags_from_json()`
- **Funci√≥n del bot√≥n:** Se ejecuta con `_process_urls()` que llama a `scrape_tags_from_json()`
- **Fuentes de datos:**
  - Desde Drive (carpeta "scraping google")
  - Desde ordenador (archivo JSON)
  - **Desde MongoDB** (colecci√≥n "URLs Google") - muestra los √∫ltimos 50 documentos con ID completo
- **Para llamar desde otros m√≥dulos:**
  ```python
  from services.tag_scraping_service import TagScrapingService
  service = TagScrapingService()
  results = await service.scrape_tags_from_json(json_data, max_concurrent, progress_callback)
  ```

#### 3. **URLs manuales**
- **Archivo UI:** `ui/pages/manual_scraping.py`
- **Clase:** `ManualScrapingPage`
- **Servicio:** `services/manual_scraping_service.py`
- **Clase del servicio:** `ManualScrapingService`
- **Funci√≥n principal:** `scrape_urls()`
- **Funci√≥n del bot√≥n:** Se ejecuta con `_execute_scraping()` que llama a `scrape_urls()`
- **Para llamar desde otros m√≥dulos:**
  ```python
  from services.manual_scraping_service import ManualScrapingService
  service = ManualScrapingService()
  results = service.scrape_urls(urls, tags, max_workers, timeout)
  ```

#### 4. **Booking.com**
- **Archivo UI:** `ui/pages/booking_scraping.py`
- **Clase:** `BookingScrapingPage`
- **Servicio:** `services/booking_scraping_service.py`
- **Clase del servicio:** `BookingScrapingService`
- **Funci√≥n principal:** `scrape_hotels()`
- **Funci√≥n del bot√≥n:** Se ejecuta con `_perform_scraping()` que llama a `scrape_hotels()`
- **Para llamar desde otros m√≥dulos:**
  ```python
  from services.booking_scraping_service import BookingScrapingService
  service = BookingScrapingService()
  results = await service.scrape_hotels(booking_urls, progress_callback)
  ```

---

## üìù SECCI√ìN CONTENIDO

#### 5. **Generador de art√≠culos**
- **Archivo UI:** `ui/pages/article_generator.py`
- **Clase:** `ArticleGeneratorPage`
- **Servicio:** `services/article_generator_service.py`
- **Clase del servicio:** `ArticleGeneratorService`
- **Funci√≥n principal:** `generate_article_schema()`
- **Funci√≥n del bot√≥n:** Se ejecuta con `_execute_generation()` que llama a `generate_article_schema()`
- **Para llamar desde otros m√≥dulos:**
  ```python
  from services.article_generator_service import ArticleGeneratorService
  service = ArticleGeneratorService()
  result = service.generate_article_schema(keyword, language, content_type, model, generate_text, generate_slug, competition_data, temperature, top_p, frequency_penalty, presence_penalty)
  ```

#### 6. **Chat GPT**
- **Archivo UI:** `ui/pages/gpt_chat.py`
- **Clase:** `GPTChatPage`
- **Servicio:** `services/chat_service.py`
- **Clase del servicio:** `ChatService`
- **Funci√≥n principal:** `generate_response()`
- **Funci√≥n del bot√≥n:** Se ejecuta autom√°ticamente al escribir en el chat input que llama a `generate_response()`
- **Para llamar desde otros m√≥dulos:**
  ```python
  from services.chat_service import ChatService
  service = ChatService()
  response = service.generate_response(messages, model, temperature, max_tokens, stream)
  ```

---

## üìä SECCI√ìN AN√ÅLISIS

#### 7. **An√°lisis sem√°ntico**
- **Archivo UI:** `ui/pages/embeddings_analysis.py`
- **Clase:** `EmbeddingsAnalysisPage`
- **Servicio:** `services/embeddings_service.py`
- **Clase del servicio:** `EmbeddingsService`
- **Funci√≥n principal:** `analyze_and_group_titles()`
- **Funci√≥n del bot√≥n:** Se ejecuta con `_execute_analysis()` que llama a `analyze_and_group_titles()`
- **Para llamar desde otros m√≥dulos:**
  ```python
  from services.embeddings_service import EmbeddingsService
  service = EmbeddingsService()
  result = service.analyze_and_group_titles(data, max_titles_h2, max_titles_h3, n_clusters_h2, n_clusters_h3, model)
  ```

---

## üîß SERVICIOS AUXILIARES

### Servicios de utilidad:
- **Drive:** `services/drive_service.py` - `DriveService`
- **MongoDB:** `repositories/mongo_repository.py` - `MongoRepository`
- **HTTP:** `services/utils/httpx_service.py` - `HttpxService`
- **Playwright:** `services/utils/playwright_service.py` - `PlaywrightService`

### Componentes UI comunes:
- **Archivo:** `ui/components/common.py`
- **Componentes:** `Alert`, `Button`, `Card`, `LoadingSpinner`, `DataDisplay`, etc.

---

## üìã RESUMEN DE FUNCIONES PRINCIPALES

| Bot√≥n | Funci√≥n Principal | Servicio | M√©todo |
|-------|------------------|----------|---------|
| URLs de Google | `search_multiple_queries()` | `GoogleScrapingService` | Scraping con BrightData API |
| Etiquetas HTML | `scrape_tags_from_json()` | `TagScrapingService` | Extracci√≥n de H1/H2/H3 |
| URLs manuales | `scrape_urls()` | `ManualScrapingService` | Scraping de etiquetas SEO |
| Booking.com | `scrape_hotels()` | `BookingScrapingService` | Extracci√≥n de datos de hoteles |
| Generador de art√≠culos | `generate_article_schema()` | `ArticleGeneratorService` | Generaci√≥n con GPT |
| Chat GPT | `generate_response()` | `ChatService` | Chat libre con GPT |
| An√°lisis sem√°ntico | `analyze_and_group_titles()` | `EmbeddingsService` | Agrupaci√≥n con embeddings |

---

## üéØ EJEMPLO DE USO DESDE OTROS M√ìDULOS

```python
# Ejemplo para usar el scraping de Google desde otro m√≥dulo
from services.google_scraping_service import GoogleScrapingService

def mi_funcion_personalizada():
    service = GoogleScrapingService()
    
    # Configurar par√°metros
    queries = ["hoteles Madrid", "restaurantes Barcelona"]
    num_results = 20
    language_code = "es"
    region_code = "es"
    google_domain = "google.es"
    
    # Ejecutar scraping
    results = service.search_multiple_queries(
        queries=queries,
        num_results=num_results,
        language_code=language_code,
        region_code=region_code,
        google_domain=google_domain
    )
    
    return results
```

---

## üèóÔ∏è ARQUITECTURA Y REUTILIZACI√ìN DE C√ìDIGO

### Principios de dise√±o:
1. **DRY (Don't Repeat Yourself)**: No hay duplicaci√≥n de c√≥digo
2. **Separaci√≥n de responsabilidades**: L√≥gica de negocio (servicios) separada de la UI (p√°ginas)
3. **Reutilizaci√≥n de componentes**: Las p√°ginas pueden importar y usar m√©todos de otras p√°ginas

### Ejemplo de reutilizaci√≥n:
La p√°gina "URLs de Google" cuando tiene el checkbox marcado:
- **L√≥gica**: Usa `TagScrapingService.scrape_tags_from_json()` (mismo servicio que Etiquetas HTML)
- **Visualizaci√≥n**: Importa `TagScrapingPage` y usa su m√©todo `_display_url_result()`

```python
# En GoogleScrapingPage.__init__()
from ui.pages.tag_scraping import TagScrapingPage
self.tag_page = TagScrapingPage()

# En GoogleScrapingPage._display_url_result()
def _display_url_result(self, url_result: Dict[str, Any]):
    """Reutiliza el m√©todo de visualizaci√≥n de la p√°gina de etiquetas HTML"""
    self.tag_page._display_url_result(url_result)
```

### Flujos de datos entre m√≥dulos:
1. **URLs de Google ‚Üí MongoDB (colecci√≥n "URLs Google")**
2. **Etiquetas HTML ‚Üí puede cargar desde MongoDB (colecci√≥n "URLs Google")**
3. **URLs de Google con checkbox ‚Üí MongoDB (colecci√≥n "hoteles")**

---

## üìÅ ESTRUCTURA DEL PROYECTO

```
serpy/
‚îú‚îÄ‚îÄ config/              # Configuraci√≥n y settings
‚îú‚îÄ‚îÄ repositories/        # Capa de acceso a datos
‚îú‚îÄ‚îÄ services/           # L√≥gica de negocio
‚îÇ   ‚îú‚îÄ‚îÄ utils/         # Utilidades (httpx, playwright)
‚îÇ   ‚îî‚îÄ‚îÄ ...            # Servicios espec√≠ficos
‚îú‚îÄ‚îÄ ui/                # Interfaz de usuario
‚îÇ   ‚îú‚îÄ‚îÄ components/    # Componentes reutilizables
‚îÇ   ‚îî‚îÄ‚îÄ pages/         # P√°ginas de la aplicaci√≥n
‚îú‚îÄ‚îÄ streamlit_app.py   # Punto de entrada
‚îú‚îÄ‚îÄ requirements.txt   # Dependencias
‚îî‚îÄ‚îÄ Dockerfile        # Configuraci√≥n Docker
```

Esta gu√≠a te proporciona toda la informaci√≥n que necesitas para entender la estructura de tu aplicaci√≥n y c√≥mo llamar a las funciones desde otros m√≥dulos.
