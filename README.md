# SERPY - Sistema de Scraping y AnÃ¡lisis SEO

SERPY es una suite completa de herramientas SEO que incluye scraping web, gestiÃ³n de imÃ¡genes y anÃ¡lisis de contenido. El proyecto estÃ¡ compuesto por tres microservicios principales que trabajan de forma integrada.

## ğŸ—ï¸ Arquitectura

El proyecto estÃ¡ organizado en tres componentes principales:

### 1. API Principal (`/api`)
API REST que proporciona acceso a las colecciones de MongoDB con detecciÃ³n dinÃ¡mica de colecciones.

**CaracterÃ­sticas:**
- DetecciÃ³n automÃ¡tica de colecciones desde MongoDB
- Endpoints RESTful para cada colecciÃ³n
- BÃºsqueda de texto completo
- PaginaciÃ³n configurable
- URLs amigables con slugs opcionales
- Respuestas JSON formateadas

**TecnologÃ­as:**
- FastAPI
- MongoDB (PyMongo)
- Python 3.8+

### 2. Servicio de ImÃ¡genes (`/images-service`)
Microservicio especializado en la descarga y gestiÃ³n de imÃ¡genes.

**CaracterÃ­sticas:**
- Descarga asÃ­ncrona de imÃ¡genes desde MongoDB o APIs externas
- Sistema de trabajos con seguimiento de estado
- Servicio de imÃ¡genes con URLs directas
- Reintentos automÃ¡ticos para descargas fallidas
- OrganizaciÃ³n estructurada de archivos

**TecnologÃ­as:**
- FastAPI
- Celery + Redis (para trabajos asÃ­ncronos)
- httpx (para descargas asÃ­ncronas)
- MongoDB (opcional)

### 3. Scraper (`/scraper`)
Interfaz web para scraping y anÃ¡lisis de contenido.

**CaracterÃ­sticas:**
- Scraping de resultados de Google
- ExtracciÃ³n de datos de Booking.com
- GeneraciÃ³n de artÃ­culos con GPT
- AnÃ¡lisis semÃ¡ntico con embeddings
- IntegraciÃ³n con Google Drive

**TecnologÃ­as:**
- Streamlit
- Selenium/Playwright
- OpenAI API
- Google Drive API

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Requisitos Previos
- Docker y Docker Compose
- Python 3.8+
- MongoDB
- Redis (para el servicio de imÃ¡genes)

### ConfiguraciÃ³n RÃ¡pida

1. **Clonar el repositorio:**
```bash
git clone https://github.com/serpy/serpy.git
cd serpy
```

2. **Configurar variables de entorno:**

Para la API:
```bash
cp api/.env.example api/.env
# Editar api/.env con tus credenciales
```

Para el servicio de imÃ¡genes:
```bash
cp images-service/.env.example images-service/.env
# Editar images-service/.env
```

3. **Iniciar con Docker Compose:**
```bash
docker-compose up -d
```

## ğŸ“š DocumentaciÃ³n de APIs

### API Principal
- **URL Base:** https://api.serpsrewrite.com
- **DocumentaciÃ³n:** https://api.serpsrewrite.com/docs (en desarrollo)

#### Endpoints principales:
- `GET /` - InformaciÃ³n de la API y colecciones disponibles
- `GET /health` - Estado de salud de la API
- `GET /collections` - Lista de colecciones
- `GET /{collection}` - Listar documentos con paginaciÃ³n
- `GET /{collection}/{id}` - Obtener documento especÃ­fico
- `GET /{collection}/search/{query}` - Buscar en colecciÃ³n

### Servicio de ImÃ¡genes
- **URL Base:** https://images.serpsrewrite.com
- **AutenticaciÃ³n:** Header `X-API-Key`

#### Endpoints principales:
- `POST /api/v1/download/from-api-url-simple` - Descargar desde API externa
- `GET /api/v1/jobs` - Listar trabajos de descarga
- `GET /api/v1/images/{database}/{collection}/{document_id}/` - Listar imÃ¡genes
- `GET /api/v1/images/{database}/{collection}/{document_id}/{filename}` - Servir imagen

## ğŸ”§ Desarrollo

### Estructura del Proyecto
```
serpy/
â”œâ”€â”€ api/                    # API principal
â”‚   â”œâ”€â”€ main.py            # AplicaciÃ³n FastAPI
â”‚   â”œâ”€â”€ config/            # ConfiguraciÃ³n
â”‚   â””â”€â”€ requirements.txt   # Dependencias
â”‚
â”œâ”€â”€ images-service/        # Servicio de imÃ¡genes
â”‚   â”œâ”€â”€ app/              # CÃ³digo de la aplicaciÃ³n
â”‚   â”‚   â”œâ”€â”€ api/          # Endpoints
â”‚   â”‚   â”œâ”€â”€ core/         # ConfiguraciÃ³n y utilidades
â”‚   â”‚   â”œâ”€â”€ models/       # Modelos de datos
â”‚   â”‚   â”œâ”€â”€ services/     # LÃ³gica de negocio
â”‚   â”‚   â””â”€â”€ workers/      # Trabajos asÃ­ncronos
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ scraper/              # Interfaz de scraping
    â”œâ”€â”€ streamlit_app.py  # AplicaciÃ³n principal
    â”œâ”€â”€ config/           # ConfiguraciÃ³n
    â”œâ”€â”€ services/         # Servicios de scraping
    â””â”€â”€ ui/               # Componentes de UI
```

### Ejecutar en Desarrollo

**API Principal:**
```bash
cd api
pip install -r requirements.txt
python main.py
```

**Servicio de ImÃ¡genes:**
```bash
cd images-service
pip install -r requirements.txt
# Terminal 1: API
python -m app.main
# Terminal 2: Worker Celery
celery -A app.workers.celery_app worker --loglevel=info
```

**Scraper:**
```bash
cd scraper
pip install -r requirements.txt
streamlit run streamlit_app.py
```

## ğŸ” Seguridad

- Todas las APIs requieren autenticaciÃ³n mediante API Key
- Las credenciales sensibles se almacenan en archivos `.env` o `secrets.json`
- CORS configurado para dominios especÃ­ficos en producciÃ³n
- Logs sanitizados para no exponer informaciÃ³n sensible

## ğŸ“Š Monitoreo

- Logs estructurados en formato JSON
- MÃ©tricas de rendimiento en cada request
- Health checks para cada servicio
- Sistema de alertas para errores crÃ­ticos

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“ Licencia

Este proyecto es privado y propietario. Todos los derechos reservados.

## ğŸ‘¥ Equipo

- **Desarrollo:** Equipo SERPY
- **Contacto:** contacto@serpsrewrite.com

---

Para mÃ¡s informaciÃ³n, consulta la documentaciÃ³n especÃ­fica de cada componente en sus respectivos directorios.
